{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define directory with context files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './context/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all files present in given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        loader = PyPDFLoader(filepath)\n",
    "        documents = loader.load()\n",
    "        all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f'Total number of chunks: {len(texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print('embedding model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Vector DB or Create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('chroma_db'):\n",
    "    print('Loading from saved db')\n",
    "    vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"chroma_db\")\n",
    "    print('document loaded')\n",
    "else:\n",
    "    print('No vector previously created db found... Creating new db')\n",
    "    vectorstore = Chroma.from_documents(texts, embeddings, persist_directory=\"chroma_db\")\n",
    "    print('document ingested')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a LLM/SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"phi3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "        You are an AI lawyer specializing in Indian law.\n",
    "        Your role is to provide clear, concise, and accurate legal advice based solely on the information from the provided documents and prior conversations with the user.\n",
    "        You must always respond as a legal expert and avoid disclaiming your expertise.\n",
    "        If an answer is unknown, simply state that and refrain from speculation.\n",
    "        Cite relevant law sections, acts, or provisions in your response.\n",
    "        Note: The developer has provided the legal documents, not the user.\n",
    "\n",
    "        Previous conversations:\n",
    "        {history}\n",
    "\n",
    "        Document context:\n",
    "        {context}\n",
    "    \"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function with Retrival Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnAsk():\n",
    "    while True:\n",
    "        query = input(\"Question:\")\n",
    "\n",
    "        print(\"User:\", query, \"\\n\")\n",
    "        if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": HumanMessage(content=query)})\n",
    "\n",
    "        if query:\n",
    "            relevant_docs = retriever.invoke(query)\n",
    "            context_documents_str = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "        else:\n",
    "            context_documents_str = \"\"\n",
    "\n",
    "        qa_prompt_local  = qa_prompt.partial(\n",
    "            history=history,\n",
    "            context=context_documents_str\n",
    "        )\n",
    "\n",
    "        llm_chain = { \"input\": RunnablePassthrough() } | qa_prompt_local  | llm\n",
    "\n",
    "        result = llm_chain.invoke(query)\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": AIMessage(content=result)})\n",
    "\n",
    "        print(\"Bot:\", result, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnAsk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
